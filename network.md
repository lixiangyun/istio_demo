# istio 组网

准备工作，可以查看[istio安装部署](README.md)，了解一下istio的bookinfo样例的服务部署。

## 容器部署

通过docker命令查看容器状态，可以知道每个服务都会四个容器运行（server、proxy_init、proxy_debug、ubuntu-tools）

例如：`bookinfo_productpage-v1-ubuntu_1` 容器，可以通过`docker ps -a | grep productpage`看到；

```
53e500560655    docker.io/istio/proxy_debug:1.0.2               "su istio-proxy -c..."   6 days ago   Up 6 days                        bookinfo_productpage-v1-sidecar_1
6f681ba44460    linimbus/ubuntu-tools:latest                    "/root/hold_on.sh"       8 days ago   Up 8 days                        bookinfo_productpage-v1-ubuntu_1
a93a152621ff    docker.io/istio/proxy_init:0.7.1                "/usr/local/bin/pr..."   8 days ago   Exited (0) 8 days ago            bookinfo_productpage-v1-init_1
01b63843939f    istio/examples-bookinfo-productpage-v1:1.8.0    "/bin/sh -c 'pytho..."   8 days ago   Up 8 days   0.0.0.0:9081->9080/tcp, 0.0.0.0:1500->15000/tcp   bookinfo_productpage-v1_1
```

这四个容器在部署脚本（bookinfo.sidecars.yaml）中可以看出，共享相同容器网络配置。即：网络配置相同，包括IP地址、hostname、net filter规则。

```

...
  productpage-v1-init:
    image: docker.io/istio/proxy_init:0.7.1
    cap_add:
      - NET_ADMIN
    network_mode: "container:bookinfo_productpage-v1_1"
    command:
      - -p
      - "15001"
      - -u
      - "1337"
  productpage-v1-sidecar:
    image: docker.io/istio/proxy_debug:1.0.2
    network_mode: "container:bookinfo_productpage-v1_1"
    entrypoint:
      - su
      - istio-proxy
      - -c
      - "/usr/local/bin/pilot-agent proxy --serviceregistry Consul --serviceCluster productpage-v1 --zipkinAddress zipkin:9411 --configPath /var/lib/istio >/tmp/envoy.log"
  productpage-v1-ubuntu:
    image: linimbus/ubuntu-tools:latest
    network_mode: "container:bookinfo_productpage-v1_1"
    privileged: true
    entrypoint:
      - "/root/hold_on.sh"
...
```

## 服务实现

productpage容器做为开发者开发的服务，按照服务本身进行运行（包括监听端口，如9080，向其他服务发起请求），[查看源码](bookinfo/src/productpage.py)。从源码中了解到productpage服务作为前端服务，根据用户（浏览器）查看url路径，然后发起后端服务的请求，如(`http://reviews.service.consul:9080/reviews/{product_id}`)，获取相关bookid的review信息。

代码片段：

- 从用户(浏览器)访问的路径，触发调用getProductReviews接口获取product_id的reviews信息。

```
@app.route('/api/v1/products/<product_id>/reviews')
def reviewsRoute(product_id):
    headers = getForwardHeaders(request)
    status, reviews = getProductReviews(product_id, headers)
    return json.dumps(reviews), status, {'Content-Type': 'application/json'}
```


- getProductReviews接口实现，从全局变量reviews构造url路径，发起http request，并校验返回值和接收json编码body，反序列化；

```
def getProductReviews(product_id, headers):
    ## Do not remove. Bug introduced explicitly for illustration in fault injection task
    ## TODO: Figure out how to achieve the same effect using Envoy retries/timeouts
    for _ in range(2):
        try:
            url = reviews['name'] + "/" + reviews['endpoint'] + "/" + str(product_id)
            res = requests.get(url, headers=headers, timeout=3.0)
        except:
            res = None
        if res and res.status_code == 200:
            return 200, res.json()
    status = res.status_code if res is not None and res.status_code else 500
    return status, {'error': 'Sorry, product reviews are currently unavailable for this book.'}
```

- 全局变量reviews为：

```
servicesDomain = "" if (os.environ.get("SERVICES_DOMAIN") == None) else "." + os.environ.get("SERVICES_DOMAIN")

reviews = {
    "name" : "http://reviews{0}:9080".format(servicesDomain),
    "endpoint" : "reviews",
    "children" : [ratings]
}
```

- 环境变量通过docker inspect 可以查看到。

## 请求截获（envoy代理）

我们可以通过ubuntu-tools容器，使用构造一个请求方式，模拟一个服务间的请求，如： `curl -v http://reviews.service.consul:9080/reviews/1`，返回结果如下：

```
*   Trying 172.28.0.9...
* Connected to reviews.service.consul (172.28.0.9) port 9080 (#0)
> GET /reviews/1 HTTP/1.1
> Host: reviews.service.consul:9080
> User-Agent: curl/7.47.0
> Accept: */*
> 
< HTTP/1.1 200 OK
< x-powered-by: Servlet/3.1
< content-type: application/json
< date: Wed, 07 Nov 2018 04:25:06 GMT
< content-language: en-US
< content-length: 379
< x-envoy-upstream-service-time: 15
< server: envoy
< 
* Connection #0 to host reviews.service.consul left intact
{"id": "1","reviews": [{  "reviewer": "Reviewer1",  "text": "An extremely entertaining play by Shakespeare. The slapstick humour is refreshing!", "rating": {"stars": 5, "color": "black"}},{  "reviewer": "Reviewer2",  "text": "Absolutely fun and entertaining. The play lacks thematic depth when compared to other plays by Shakespeare.", "rating": {"stars": 4, "color": "black"}}]}
```

我们注意到请求相应的head部分有envoy的键值对，说明请求经过了envoy代理。但是看url路径明显不是发送给envoy的。通过`iptalbes-save` 导出当前的net filter规则如下：

    ```
    # Generated by iptables-save v1.6.0 on Wed Nov  7 03:51:05 2018
    *nat
    :PREROUTING ACCEPT [1050:275440]
    :INPUT ACCEPT [1094:277920]
    :OUTPUT ACCEPT [301421:20927026]
    :POSTROUTING ACCEPT [598307:37109939]
    :DOCKER_OUTPUT - [0:0]
    :DOCKER_POSTROUTING - [0:0]
    :ISTIO_OUTPUT - [0:0]
    :ISTIO_REDIRECT - [0:0]
    -A PREROUTING -m comment --comment "istio/install-istio-prerouting" -j ISTIO_REDIRECT
    -A OUTPUT -d 127.0.0.11/32 -j DOCKER_OUTPUT
    -A OUTPUT -p tcp -m comment --comment "istio/install-istio-output" -j ISTIO_OUTPUT
    -A POSTROUTING -d 127.0.0.11/32 -j DOCKER_POSTROUTING
    -A DOCKER_OUTPUT -d 127.0.0.11/32 -p tcp -m tcp --dport 53 -j DNAT --to-destination 127.0.0.11:38786
    -A DOCKER_OUTPUT -d 127.0.0.11/32 -p udp -m udp --dport 53 -j DNAT --to-destination 127.0.0.11:35949
    -A DOCKER_POSTROUTING -s 127.0.0.11/32 -p tcp -m tcp --sport 38786 -j SNAT --to-source :53
    -A DOCKER_POSTROUTING -s 127.0.0.11/32 -p udp -m udp --sport 35949 -j SNAT --to-source :53
    -A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -m comment --comment "istio/redirect-implicit-loopback" -j ISTIO_REDIRECT
    -A ISTIO_OUTPUT -m owner --uid-owner 1337 -m comment --comment "istio/bypass-envoy" -j RETURN
    -A ISTIO_OUTPUT -d 127.0.0.1/32 -m comment --comment "istio/bypass-explicit-loopback" -j RETURN
    -A ISTIO_OUTPUT -m comment --comment "istio/redirect-default-outbound" -j ISTIO_REDIRECT
    -A ISTIO_REDIRECT -p tcp -m comment --comment "istio/redirect-to-envoy-port" -j REDIRECT --to-ports 15001
    COMMIT
    # Completed on Wed Nov  7 03:51:05 2018
    # Generated by iptables-save v1.6.0 on Wed Nov  7 03:51:05 2018
    *filter
    :INPUT ACCEPT [6901719:11778509389]
    :FORWARD ACCEPT [0:0]
    :OUTPUT ACCEPT [6323128:1240278836]
    COMMIT
    # Completed on Wed Nov  7 03:51:05 2018
    ```

注：基本上所有服务的iptables规则都是一致的，并且规则是逐一匹配，详细iptables原理可以自行了解。

**ingress：**
规则第一条
`-A PREROUTING -m comment --comment "istio/install-istio-prerouting" -j ISTIO_REDIRECT` ，表示所有路由前的请求，重定向到`ISTIO_REDIRECT`，然后最后一条规则`-A ISTIO_REDIRECT -p tcp -m comment --comment "istio/redirect-to-envoy-port" -j REDIRECT --to-ports 15001`表示tcp的协议，重定向到15001端口。这个端口就是envoy监听的端口。

通过`netstat -nap`，可以看到监听的端口列表：
```
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.11:38786        0.0.0.0:*               LISTEN      -               
tcp        0      0 127.0.0.1:15000         0.0.0.0:*               LISTEN      -               
tcp        0      0 0.0.0.0:9080            0.0.0.0:*               LISTEN      -               
tcp        0      0 0.0.0.0:15001           0.0.0.0:*               LISTEN      -               
udp        0      0 127.0.0.11:35949        0.0.0.0:*                           -   
```

**outgress：**
出去的流量截获规则比较复杂，首先看到OUTPUT链的规则：
```
-A OUTPUT -d 127.0.0.11/32 -j DOCKER_OUTPUT
-A OUTPUT -p tcp -m comment --comment "istio/install-istio-output" -j ISTIO_OUTPUT
```

有两个OUTPUT链，第一条规则制定目的IP网段是 `127.0.0.11/32`的跳转到 `DOCKER_OUTPUT`，然后我们再看`DOCKER_OUTPUT`规则；
```
-A DOCKER_OUTPUT -d 127.0.0.11/32 -p tcp -m tcp --dport 53 -j DNAT --to-destination 127.0.0.11:38786
-A DOCKER_OUTPUT -d 127.0.0.11/32 -p udp -m udp --dport 53 -j DNAT --to-destination 127.0.0.11:35949
```

此时可以看出所有（tcp & udp）的目的端口（53），通过DNAT指定到 `127.0.0.11:38786` 和 `127.0.0.11:35949`，因为53是DNS域名解析的知名端口，所以相当于容器内DNS解析的报文都会根据协议转发 `tcp:127.0.0.11:38786` 和 `udp:127.0.0.11:35949` 这两个域名解析服务。而前面`netstat -nap`命令可以看出监听端口。这个应该属于容器自带的DNS解析服务，我们暂时不深入研究。

第二个OUTPUT链的规则是，tcp协议的跳转到 `ISTIO_OUTPUT`，然后我们看下`ISTIO_OUTPUT`规则：

```
-A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -m comment --comment "istio/redirect-implicit-loopback" -j ISTIO_REDIRECT
-A ISTIO_OUTPUT -m owner --uid-owner 1337 -m comment --comment "istio/bypass-envoy" -j RETURN
-A ISTIO_OUTPUT -d 127.0.0.1/32 -m comment --comment "istio/bypass-explicit-loopback" -j RETURN
-A ISTIO_OUTPUT -m comment --comment "istio/redirect-default-outbound" -j ISTIO_REDIRECT
```

`ISTIO_OUTPUT`规则总共有四条，第一条表示目的端口不是`127.0.0.1/32`和`lo网口`的跳转到 `STIO_REDIRECT`，第二条规则指定uid为1337的结束匹配。我们从proxy-debug容器可以了解到，启动envoy的uid其实就是1337，说明`ISTIO_OUTPUT`规则对于envoy不起作用。这个很容易理解，就是iptables规则主要是作用于app服务。对于envoy代理进程需要做到隔离。后面两条规则和第一条看似重复（没有揣摩清楚原因）。OUTPUT链重定向到`ISTIO_REDIRECT`，和前面的ingress一样，转发到了envoy代理进程，由envoy代理进程负责转发。


综上所述：所有服务的ingress和outgress流量，都会被envoy代理进行截获。并且由envoy代理进行负责转发请求。符合service mesh思想。


## envoy路由转发

我们从前面的banchmark的静态部署场景了解到。envoy需要配置相应的规则才可以实现转发。而在istio的场景下面，是如何实现转发的？

首先我们看下envoy进行如何启动，`docker ps -a | grep proxy_debug` 选择一个容器ID，进入容器然后 `ps -ef`

```
UID         PID   PPID  C STIME TTY          TIME CMD
root          1      0  0 Oct31 ?        00:00:00 su istio-proxy -c /usr/local/bin/pilot-agent proxy --serviceregistry Consul --serviceCluster productpage-v1 --zipkinAddress zipkin:9411 --configPath /var/lib/ist
istio-p+      6      1  0 Oct31 ?        00:00:00 bash -c /usr/local/bin/pilot-agent proxy --serviceregistry Consul --serviceCluster productpage-v1 --zipkinAddress zipkin:9411 --configPath /var/lib/istio >/tmp/e
istio-p+      7      6  0 Oct31 ?        00:00:11 /usr/local/bin/pilot-agent proxy --serviceregistry Consul --serviceCluster productpage-v1 --zipkinAddress zipkin:9411 --configPath /var/lib/istio
root         40      0  0 Oct31 ?        00:00:00 /bin/bash
istio-p+     58      7  0 Oct31 ?        01:19:18 /usr/local/bin/envoy -c /var/lib/istio/envoy-rev0.json --restart-epoch 0 --drain-time-s 2 --parent-shutdown-time-s 3 --service-cluster productpage-v1 --service-n
...
```

我们可以看到 `pilot-agent` 和 `envoy` 两个进程，还有其他执行脚本的 `bash` 进程。从容器的启动脚本看，应该是先启动  `pilot-agent`，然后 `pilot-agent` 进程拉起脚本执行envoy。在envoy进程启动选项中，可以看到 `/var/lib/istio/envoy-rev0.json` 文件，打开这个文件可以看到许多有用的信息，比如xds的配置（istio控制面的endpoint）没有看到路由和集群发现的配置。从envoy官方手册得知，envoy支持通过grpc接口下发动态配置。而这个文件只是envoy启动的基本配置。

[envoy-rev0.json](./envoy-rev0.json)

```
...
	"admin": {
		"access_log_path": "/dev/stdout",
		"address": {
			"socket_address": {
				"address": "127.0.0.1",
				"port_value": 15000
			}
		}
	}
...
			"lb_policy": "ROUND_ROBIN",
			"hosts": [{
				"socket_address": {
					"address": "istio-pilot",
					"port_value": 15010
				}
			}],
...
	"tracing": {
		"http": {
			"name": "envoy.zipkin",
			"config": {
				"collector_cluster": "zipkin"
			}
		}
	}
...
```

在基本配置中，我们可以看到admin管理端口，可以通过15000端口访问。通过`curl -v http://127.0.0.1:15000/config_dump`可以导出相应的动态配置（[config-dump.json](./istio_vm/config-dump.json)）。比较重要的信息，包括envoy监听的15001端口。服务的路由配置等等，片段如下；

```
    "name": "virtual",
    "address": {
    	"socket_address": {
    		"address": "0.0.0.0",
    		"port_value": 15001
    	}
    },

```

```
	"name": "reviews.service.consul:9080",
	"domains": ["reviews.service.consul",
	"reviews.service.consul:9080",
	"reviews",
	"reviews:9080",
	"reviews.service",
	"reviews.service:9080"],
	"routes": [{
		"match": {
			"prefix": "/"
		},
		"route": {
			"cluster": "outbound|9080||reviews.service.consul",
			"timeout": "0s",
			"max_grpc_timeout": "0s"
		},
```

总结：envoy通过istio控制面下发的配置，实现服务路由和发现特性。

## istio控制面

在istio在容器部署时，控制面服务有（etcd、kube-apiserver、consul、pilot、zipkin、registrator）

服务间依赖关系示意图：

- etcd：kv存储服务，做为kube的配置存储服务。
- kube-apiserver：k8s的api网关，负责istio控制面路由策略配置。通过kubectl命令操作。
- consul：注册中心，负责所有服务状态的记录。
- pilot：istio控制面一部分，负责服务发现，路由策略下发等等。主要是通过consul、kube-apiserver获得的服务发现和路由策略，通过grpc协议下发到envoy代理进行中。
- zipkin：调用链跟踪的记录服务端，envoy代理启用调用链跟踪时，会将调用链记录发送至zipkin。
- registrator：容器注册服务，定时从dockerd获取所有容器状态，并将服务注册到consul注册中心。

